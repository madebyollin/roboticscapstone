<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Aâ€¢Robotics</title>
    <link rel="stylesheet" href="/css/basics.css">
    <link rel="stylesheet" href="/css/main.css">
    <link rel="icon" href="/favicon.png">
</head>

<body class="post">
    <a href="/"><img src="/favicon.png" width="128px" id="postHeaderLogo"/></a>
    <h1>Blog Post #9 Part 4</h1>
    <h2>Progress Update</h2>
    <main>
        <h2>Navigation</h2>
        <p>This week, we got the robot to work with the app to navigate between a start position and a destination position. We made some changes to make the robot more capable of finding a path. Additionally, we slowed the robot down so that it moves at
            about half of its maximum speed when going forward and turning.</p>
        <p><strong>Worked on:</strong> Kennan, Yifan</p>

        <h2>Speech</h2>
        <p>Previously, the robot was turning too quickly and it was hard to follow. We thought this would be difficult for a blind person to use. To solve this problem, if the robot is making a sharp turn, it will speak "turning right" or "turning left"
            so that everyone around it knows what it's doing. We used the sound tutorial to do this.</p>
        <p><strong>Worked on:</strong> Kennan, Yifan</p>

        <h2>Pressure</h2>
        <p>We got the pressure sensor working with Arduino, then connected the Arduino to the robot. We then run code on the robot that reads the pressure data and publishes it. Our main navigation ros node subscribes to this pressure publisher and uses
            the data to make navigation more responsive to the user's state. When the user sends a navigation request in the app, the robot won't start moving until it feels pressure. Then, if the user lets go in the middle of navigation, the robot will
            stop moving until the user takes its hand again.</p>
        <p><strong>Worked on:</strong> Kennan, Ollin, Yifan, Ghada</p>

        <h2>Perception</h2>
        <p>
          Our robot can find AR tags that is displayed on the phone when
          it is within the robot's peripheral vision. The robot computes the pose
          of the AR tag relative to its base and reaches its gripper about 20 cm
          away from the AR tag. We also developed our perception algorithm to be able to find
          the AR tag when the AR tag is not visible in its peripheral vision.
          We are
          also currently working on moving the gripper slower so that it does not harm
          the user if the gripper makes contact with the user's hand. We will also
          pre-record a motion so that we can easily start navigating the user.
        </p>
        <p><strong>Worked on:</strong> Ollin, Kevin<p>
    </main>
</body>

</html>
